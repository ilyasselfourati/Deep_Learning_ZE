{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font color=yellow><i> Deep Learning </i></font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Import$ $Libraries$ :"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 14,
=======
   "execution_count": 1,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Data$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 1:</font></b>\n",
    " \n",
    "- Import **data** (pandas dataframe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_semicolon</th>\n",
       "      <th>char_freq_leftbrac</th>\n",
       "      <th>char_freq_leftsquarebrac</th>\n",
       "      <th>char_freq_exclaim</th>\n",
       "      <th>char_freq_dollar</th>\n",
       "      <th>char_freq_pound</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_semicolon  \\\n",
       "0             0.00            0.00  ...                 0.00   \n",
       "1             0.00            0.94  ...                 0.00   \n",
       "2             0.64            0.25  ...                 0.01   \n",
       "3             0.31            0.63  ...                 0.00   \n",
       "4             0.31            0.63  ...                 0.00   \n",
       "\n",
       "   char_freq_leftbrac  char_freq_leftsquarebrac  char_freq_exclaim  \\\n",
       "0               0.000                       0.0              0.778   \n",
       "1               0.132                       0.0              0.372   \n",
       "2               0.143                       0.0              0.276   \n",
       "3               0.137                       0.0              0.137   \n",
       "4               0.135                       0.0              0.135   \n",
       "\n",
       "   char_freq_dollar  char_freq_pound  capital_run_length_average  \\\n",
       "0             0.000            0.000                       3.756   \n",
       "1             0.180            0.048                       5.114   \n",
       "2             0.184            0.010                       9.821   \n",
       "3             0.000            0.000                       3.537   \n",
       "4             0.000            0.000                       3.537   \n",
       "\n",
       "   capital_run_length_longest  capital_run_length_total  spam  \n",
       "0                          61                       278     1  \n",
       "1                         101                      1028     1  \n",
       "2                         485                      2259     1  \n",
       "3                          40                       191     1  \n",
       "4                          40                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam=pd.read_csv('spam.csv')\n",
    "df_spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 2:</font></b>\n",
    " \n",
    "- Separate the output (the column `spam`) and the input (the other columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_spam['spam']\n",
    "X=df_spam.drop('spam',axis=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 8,
=======
   "execution_count": 4,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_semicolon</th>\n",
       "      <th>char_freq_leftbrac</th>\n",
       "      <th>char_freq_leftsquarebrac</th>\n",
       "      <th>char_freq_exclaim</th>\n",
       "      <th>char_freq_dollar</th>\n",
       "      <th>char_freq_pound</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  \\\n",
       "0             0.00            0.00  ...                   0.0   \n",
       "1             0.00            0.94  ...                   0.0   \n",
       "2             0.64            0.25  ...                   0.0   \n",
       "3             0.31            0.63  ...                   0.0   \n",
       "4             0.31            0.63  ...                   0.0   \n",
       "\n",
       "   char_freq_semicolon  char_freq_leftbrac  char_freq_leftsquarebrac  \\\n",
       "0                 0.00               0.000                       0.0   \n",
       "1                 0.00               0.132                       0.0   \n",
       "2                 0.01               0.143                       0.0   \n",
       "3                 0.00               0.137                       0.0   \n",
       "4                 0.00               0.135                       0.0   \n",
       "\n",
       "   char_freq_exclaim  char_freq_dollar  char_freq_pound  \\\n",
       "0              0.778             0.000            0.000   \n",
       "1              0.372             0.180            0.048   \n",
       "2              0.276             0.184            0.010   \n",
       "3              0.137             0.000            0.000   \n",
       "4              0.135             0.000            0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       278  \n",
       "1                      1028  \n",
       "2                      2259  \n",
       "3                       191  \n",
       "4                       191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
<<<<<<< HEAD:TP1.ipynb
     "execution_count": 8,
=======
     "execution_count": 4,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 3:</font></b>\n",
    " \n",
    "- Split our data into **train data** (80% -- 3680 rows) and **test data** (921 rows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Building$ $our$ $Model$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 1:</font></b>\n",
    " \n",
    "- Creation of our model using **Keras**."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 11,
=======
   "execution_count": 6,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(100, activation='relu', input_shape=[57], name='hidden_layer1'),\n",
    "    layers.Dropout(0.3),  # Ajout d'une couche Dropout pour réduire le surajustement\n",
    "    layers.Dense(100, activation='relu', name='hidden_layer2'),\n",
    "    layers.Dropout(0.3),  # Ajout d'une couche Dropout pour réduire le surajustement\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid', name='output_layer'),  # Changement de l'activation en 'sigmoid'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 12,
=======
   "execution_count": 7,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer1 (Dense)       (None, 100)               5800      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_layer2 (Dense)       (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
<<<<<<< HEAD:TP1.ipynb
      " batch_normalization (Batch  (None, 100)               400       \n",
      " Normalization)                                                  \n",
=======
      " batch_normalization (BatchN  (None, 100)              400       \n",
      " ormalization)                                                   \n",
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16401 (64.07 KB)\n",
      "Trainable params: 16201 (63.29 KB)\n",
      "Non-trainable params: 200 (800.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 2:</font></b>\n",
    " \n",
    "- Precise the `optimizer`, the `loss-fonction` & the `metric`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": null,
=======
   "execution_count": 8,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 3:</font></b>\n",
    " \n",
    "- Train the model."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": null,
=======
   "execution_count": 9,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELITEBOOK\\anaconda3\\envs\\ner\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 3s 14ms/step - loss: 0.6796 - accuracy: 0.6552 - val_loss: 0.6040 - val_accuracy: 0.6671\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6240 - accuracy: 0.6899 - val_loss: 0.5860 - val_accuracy: 0.6739\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.7052 - val_loss: 0.5882 - val_accuracy: 0.6658\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.5699 - accuracy: 0.7103 - val_loss: 0.5801 - val_accuracy: 0.6726\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.5622 - accuracy: 0.7208 - val_loss: 0.5867 - val_accuracy: 0.6603\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.5372 - accuracy: 0.7293 - val_loss: 0.6137 - val_accuracy: 0.6332\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.5314 - accuracy: 0.7293 - val_loss: 0.5656 - val_accuracy: 0.6685\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.4898 - accuracy: 0.7578 - val_loss: 0.5766 - val_accuracy: 0.6386\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.4736 - accuracy: 0.7734 - val_loss: 0.5391 - val_accuracy: 0.6644\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.4486 - accuracy: 0.7959 - val_loss: 0.5017 - val_accuracy: 0.7147\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.4114 - accuracy: 0.8067 - val_loss: 0.4995 - val_accuracy: 0.7120\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.4067 - accuracy: 0.8220 - val_loss: 0.4514 - val_accuracy: 0.7690\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.3862 - accuracy: 0.8359 - val_loss: 0.4864 - val_accuracy: 0.7188\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.3698 - accuracy: 0.8366 - val_loss: 0.4271 - val_accuracy: 0.7867\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.3575 - accuracy: 0.8444 - val_loss: 0.4050 - val_accuracy: 0.8125\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.3470 - accuracy: 0.8522 - val_loss: 0.3715 - val_accuracy: 0.8356\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.3386 - accuracy: 0.8594 - val_loss: 0.3744 - val_accuracy: 0.8356\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.3079 - accuracy: 0.8764 - val_loss: 0.3808 - val_accuracy: 0.8139\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.3110 - accuracy: 0.8774 - val_loss: 0.3354 - val_accuracy: 0.8668\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.3007 - accuracy: 0.8743 - val_loss: 0.3151 - val_accuracy: 0.8818\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2970 - accuracy: 0.8794 - val_loss: 0.3219 - val_accuracy: 0.8777\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.2872 - accuracy: 0.8879 - val_loss: 0.2907 - val_accuracy: 0.9035\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2703 - accuracy: 0.8862 - val_loss: 0.3134 - val_accuracy: 0.8845\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2783 - accuracy: 0.8988 - val_loss: 0.2977 - val_accuracy: 0.8913\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2693 - accuracy: 0.8930 - val_loss: 0.2938 - val_accuracy: 0.8872\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2553 - accuracy: 0.9076 - val_loss: 0.3033 - val_accuracy: 0.8723\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2510 - accuracy: 0.9059 - val_loss: 0.2944 - val_accuracy: 0.8832\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2629 - accuracy: 0.9059 - val_loss: 0.2744 - val_accuracy: 0.9076\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2493 - accuracy: 0.9076 - val_loss: 0.2541 - val_accuracy: 0.9076\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2572 - accuracy: 0.8974 - val_loss: 0.2469 - val_accuracy: 0.9171\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2485 - accuracy: 0.9090 - val_loss: 0.2522 - val_accuracy: 0.9130\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.2484 - accuracy: 0.9015 - val_loss: 0.2893 - val_accuracy: 0.8859\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2339 - accuracy: 0.9076 - val_loss: 0.2298 - val_accuracy: 0.9239\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2353 - accuracy: 0.9117 - val_loss: 0.2426 - val_accuracy: 0.9144\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.2300 - accuracy: 0.9130 - val_loss: 0.2295 - val_accuracy: 0.9198\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2262 - accuracy: 0.9141 - val_loss: 0.2464 - val_accuracy: 0.9130\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2277 - accuracy: 0.9188 - val_loss: 0.2374 - val_accuracy: 0.9158\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2309 - accuracy: 0.9158 - val_loss: 0.2330 - val_accuracy: 0.9171\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2072 - accuracy: 0.9260 - val_loss: 0.2255 - val_accuracy: 0.9266\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2112 - accuracy: 0.9273 - val_loss: 0.2404 - val_accuracy: 0.9226\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2079 - accuracy: 0.9283 - val_loss: 0.2369 - val_accuracy: 0.9171\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2211 - accuracy: 0.9175 - val_loss: 0.2351 - val_accuracy: 0.9144\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2111 - accuracy: 0.9209 - val_loss: 0.2271 - val_accuracy: 0.9171\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2105 - accuracy: 0.9266 - val_loss: 0.2390 - val_accuracy: 0.9130\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.2181 - accuracy: 0.9188 - val_loss: 0.2208 - val_accuracy: 0.9253\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2015 - accuracy: 0.9280 - val_loss: 0.2147 - val_accuracy: 0.9280\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2018 - accuracy: 0.9280 - val_loss: 0.2258 - val_accuracy: 0.9171\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2109 - accuracy: 0.9246 - val_loss: 0.2121 - val_accuracy: 0.9253\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.2086 - accuracy: 0.9229 - val_loss: 0.2313 - val_accuracy: 0.9185\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.1991 - accuracy: 0.9280 - val_loss: 0.2441 - val_accuracy: 0.9022\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train,\n",
    "                 y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=50,\n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>PS :</font></b> The accuracy of our model on the validation data is **90%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model =  keras.Sequential([\n",
    "    layers.Dense(100, activation='relu', input_shape=[57], name='hidden_layer1'),\n",
    "    layers.Dropout(0.3),  # Ajout d'une couche Dropout pour réduire le surajustement\n",
    "    layers.Dense(100, activation='relu', name='hidden_layer2'),\n",
    "    layers.Dropout(0.3),  # Ajout d'une couche Dropout pour réduire le surajustement\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "])\n",
    "  model.add(keras.layers.Dense(\n",
    "      hp.Choice('units', [8, 16, 32]),\n",
    "      activation='relu'))\n",
    "  model.add(keras.layers.Dense(1, activation='sigmoid', name='output_layer') )# Changement de l'activation en 'sigmoid'\n",
    "  model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zaiss\\OneDrive\\Documents\\GitHub\\Deep_Learning_ZE\\TP1.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zaiss/OneDrive/Documents/GitHub/Deep_Learning_ZE/TP1.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuner\u001b[39m.\u001b[39msearch(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, validation_data\u001b[39m=\u001b[39m(x_val, y_val))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zaiss/OneDrive/Documents/GitHub/Deep_Learning_ZE/TP1.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Evaluate$ $Our$ $Model$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 1:</font></b>\n",
    " \n",
    "- Transform our data into a numpy/list data."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 15,
=======
   "execution_count": 10,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.values\n",
    "y_test=y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"skyblue\">Step 2:</font></b>\n",
    " \n",
    "- Evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zaiss\\OneDrive\\Documents\\GitHub\\Deep_Learning_ZE\\TP1.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zaiss/OneDrive/Documents/GitHub/Deep_Learning_ZE/TP1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zaiss/OneDrive/Documents/GitHub/Deep_Learning_ZE/TP1.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoss on test data:\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zaiss/OneDrive/Documents/GitHub/Deep_Learning_ZE/TP1.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy on test data:\u001b[39m\u001b[39m'\u001b[39m, accuracy)\n",
      "File \u001b[1;32mc:\\Users\\zaiss\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\zaiss\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3954\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   3949\u001b[0m     \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[0;32m   3950\u001b[0m     \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[0;32m   3951\u001b[0m     \u001b[39m# model is compiled\u001b[39;00m\n\u001b[0;32m   3952\u001b[0m     \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[0;32m   3953\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[1;32m-> 3954\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3955\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3956\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3957\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3958\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.9077\n",
      "Loss on test data: 0.24829959869384766\n",
      "Accuracy on test data: 0.907709002494812\n"
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss on test data:', loss)\n",
    "print('Accuracy on test data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Predict$ :"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:TP1.ipynb
   "execution_count": null,
=======
   "execution_count": 13,
>>>>>>> 7c700942309ad1c2af5a8d8ff4cbbe3381db57f7:SPAM.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/29 [>.............................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 8ms/step\n",
      "Predicted probabilities: [[0.09534885]\n",
      " [0.02020525]\n",
      " [0.0608342 ]\n",
      " [0.5988094 ]\n",
      " [0.05349546]\n",
      " [0.9279734 ]\n",
      " [0.00424988]\n",
      " [0.15510051]\n",
      " [0.00731156]\n",
      " [0.1045875 ]]\n",
      "Predicted labels: [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Original Labels : [0 0 0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for each class\n",
    "predictions_proba = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted labels (0 or 1 in this binary classification case)\n",
    "predictions = (predictions_proba > 0.5).astype(int)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predicted probabilities:\", predictions_proba[:10])  # Display the predicted probabilities for the first 10 samples\n",
    "print(\"Predicted labels:\", predictions[:10])  # Display the predicted labels for the first 10 samples\n",
    "print(\"Original Labels :\", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
